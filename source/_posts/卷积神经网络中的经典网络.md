---
title: 卷积神经网络中的经典网络
date: 2019-02-23 18:02:26
categories:
- 深度学习
tags:
- 卷积神经网络
---

​	本文主要总结卷积神经网络结构的经典结构。

<!--more-->

​	首先简要介绍以下六种网络的概要。

> 下面两张图片来自 [guoyaohua](https://www.cnblogs.com/guoyaohua/p/8534077.html)。

![](https://ws1.sinaimg.cn/large/007BZfbEly1g0ginfufroj30p40aft9f.jpg)

![](https://ws1.sinaimg.cn/large/007BZfbEly1g0gingb77nj30nn09vdhy.jpg)



# LeNet——第一个卷积神经网络

- LeCun在1989年提出的网络结构，是CNN的鼻祖。
- 网络结构和现在的 conv->pool->ReLU 不同，当时是conv1->pool1->conv2->pool 然后是全连接层

![LeNet](https://ws1.sinaimg.cn/large/007BZfbEly1g0gi2nc13lj31by0d0abm.jpg)

- 但是这种先卷积+池化，后经过全连接层的思路奠定下来

- LeNet-5解决了手写数字识别问题

  > MNIST，Mixed National Institute of Standards and Technology，
  >
  > MNIST是Hello World 级别的数据
  >
  > MNIST是NIST数据机的一个子集
  >
  > 一共有 70000 个样本，60000个为训练集，10000个为测试集
  >
  > 样本为28x28的灰度图，目的是区分 0～9，是一个十分类问题

  

# AlexNet——开启卷积网络新时代

- 经典网络，在ImageNet比赛中一举开创辉煌
- AlexNet是后来ZF-Net和VGGNet的鼻祖

## 网络结构

- AlexNet针对的是ILSVRC的分类问题，输入图片是256x256的三通道彩色图片

- 为了增强泛化能力，训练的时候Alex采用了数据增加手段包含位置随机裁剪，就是在 256x256的图片中随机产生一块224x224的子区域，所以输入的维度是 3x224x224

- 卷积的时候采用了分组的方法，也就是在 conv2和conv4->conv5的两个分叉，因为当时的GPU不够强大

- 在dropout的使用上是在两个全连接层 fc6 和 fc7

  ![](https://ws1.sinaimg.cn/large/007BZfbEly1g0gibw2zfoj316k0kjq8t.jpg)

## 局部响应归一化 LRN

- AlexNet 的改进不仅在网络的深度，而且在于以下几点
  - ReLU、Dropout和局部响应归一化
- 局部响应归一化湿在某一层得到了多通道的响应图之后，对响应图上某一位置和临近通道的指按照如下公式做归一化

![](https://ws1.sinaimg.cn/large/007BZfbEly1g0gifmbvaxj30l004mmx3.jpg)



# ZF-Net

* 基于AlexNet进行微调
* Top5错误率11.2%
* 使用ReLU激活函数和交叉熵损失函数

一些特点：

* 使用反卷积，可视化feature map。

> 转置卷积（Transposed Convolution）又称为反卷积（Deconvolution）:
>
> 推荐[本文](https://blog.csdn.net/isMarvellous/article/details/80087705)的图形解释
>
> 转置卷积形式上就和一个卷积层的反向梯度计算相同

* 与AlexNet相比，前面的层使用了更小的卷积核和更小的步长，保留了更多特征
* 通过遮挡，找出决定图像类别的关键部位。通过实验说明深度增加的时候，网络可以学习到更具区分的特征
* 网络训练时，底层参数收敛快，越到高层，则需要越长的时间训练

# VGG16

- VGGNet探索卷积神经网络的深度与其性能之间的关系，通过反复堆叠3*3的小型卷积核和2*2的最大池化层，
- VGGNet构筑了16~19层深的卷积神经网络。VGGNet相比之前state-of-the-art的网络结构，错误率大幅下降， 
- VGGNet论文中全部使用了3*3的小型卷积核和2*2的最大池化核，通过不断加深网络结构来提升性能。

![](https://ws1.sinaimg.cn/large/007BZfbEly1g0gnqsnxp9j30ih0ar79e.jpg)

* VGG-16网络中的16代表的含义为：含有参数的有16个层，共包含参数约为1.38亿。
* VGG-16网络结构很规整，没有那么多的超参数，专注于构建简单的网络，都是几个卷积层后面跟一个可以压缩
* 优点：简化卷积神经网络的结构，缺点：训练的特征数量非常大
* 随着网络的加深，图像的宽度和高度都在以一定的规律不断减小

# GoogleNet

**创新点：**

* 将层数推进到22层，并且接近了人类在ImageNet数据上的识别水平
* 跳出了AlexNet的基本结构，提出了Inception模块

## 1x1卷积，Network in Network，全局平均池化

​	1x1卷积相当于给feature map每个值都乘以了同一个权值？？

​	在单通道的时候，情况却是如此，但是在多通道的时候，就不一样了。

​	在多通道的时候，我们关注feature map上某一个位置的一组**m个像素**，在经过1x1卷积之后我们得到了输出的一组**n个像素**，如果只看特定位置上的这些像素时，其实就相当于一个 全连接层。我们在经过一组线性变换之后:

​	如果m>n,那么相当于给feature map 降维，我们就可以只在这n个feature map上做卷积，减少了计算量，如果再加上一个激活函数，那么就相当于一个感知机。如果我们再进行一组1x1的卷积 + 激活函数，那么就相当于我们将这一组位置的像素作为输出，形成了一个小的神经网络，这也就是 Network in Network。

​	那么什么是全局平均池化呢？

​	我们对最后一层卷积的响应图，每个通道求整个响应图的均值。然后再进行一层全连接，因为全局池化后相当于一个像素，那么最后的全连接其实就相当于一个加权相加。这种结构比起直接的全连接更加直观而且泛化效果更好。

## Inception

​	What is Inception？跳出AlexNet的基本结构。（这里说的是Inceptionv1）

​	Inception的基本思想源于NIN，如果把**卷积+激活**当作一种广义的线性模型Generalized Linear Model。也就是用更有效的结构代替单纯的**卷积+激活**操作。

​	Inception主要做了两件事：

1. 通过3x3池化，以及1x1、3x3、5x5三种不同尺度的卷积核，一共四种方式对输入的 feature map做了特征抽取。
2. 为了降低计算量，同时让信息通过更少的连接传递以达到更加稀疏的特性，采用了1x1卷积核进行降维。

![](https://ws1.sinaimg.cn/large/007BZfbEly1g0grxjrcc0j30ss0g4wgr.jpg)

## 网络结构

​	计算层数的时候只按照有需要训练的参数算作一层，则每个Inception中有两层，一共九个Inception层，所以是18层，其他层里则是三个卷积层加Softmax 的全连接层，一共22层。

​	优化处：

* 结构中多出三个loss单元，也就是计算代价函数的对应单元，这样做是为了帮助网络收敛。
  * 虽然ReLU能够一定程度解决梯度消失的问题，但是难以解决深层网络中难以训练的问题，越远离输出的层训练起来就没有距离输出层近的效果好。
  * 在中间加入三个loss单元，让计算损失的时候低层的特征也能有很好的区分能力。加速网络的收敛。
* 最后一个Inception模块输出7x7的832通道的feature map 之后，并没有像AlexNet那样降维然后经过两个全连接层，而是对每个feature map求了平均值，得到一个1024维的向量，再经过一个全连接的得到和输出数目对应的1000维向量用于分类，这是舍弃了经典的最后一/二全连接的做法。通过这种方式去掉全连接后，不仅参数量减少了，而且准确率还有提升。

![](https://ws1.sinaimg.cn/mw690/007BZfbEly1g0gs11i37ij315o4fy4ao.jpg)

## 批归一化 Batch Normalization

​	顾名思义，也就是对每一个batch的数据进行归一化，公式如下，应该能直接看懂：

![](https://ws1.sinaimg.cn/large/007BZfbEly1g0gsgx8zyaj30db09lta8.jpg)

​	这样能够让数据移到中心区域，可以看作是对抗梯度消失的一种手段。

​	BN的本质就是利用优化变一下方差大小和均值的位置。训练模型的时候，数据的均值和方差应该尽量贴近所有数据的分布，所以在训练过程中记录大量数据的均值和方差，得到整个训练样本的均值和方差期望值，训练结束后作为最后使用的均值和方差。

# ResNet

## 深层网络训练的难题——退化问题

​	退化问题简单来说，随着层数的加深到一定程度之后，越深的网络反而效果越差，并不是因为更深的网络造成了过拟合，也未必是因为梯度传播的衰减。而随着网络层数的加深，训练效果真的变差了。

​	考虑一个训练好的网络结构，如果随着层数加深的时候，不是单纯的堆叠更多的层，而是堆上去一层，使得堆叠后的层的输出和堆叠前的输出相同，也就是单位映射，然后继续训练。比方说用单位矩阵初始化，卷积考虑就是分通道的1x1卷积核，初始化值为1。这种情况下不应该效果更差，因为训练之前已经达到了加层数之前的水平作为初始了，然而实验结果在网络层数达到一定深度后，结果变的更差。

## 残差单元

​	残差 指的是预测值和观测值的差异。

​	误差 指的是观测值和真实值的差异。

> 残差模块：
>
> 大体思路是，既然单位映射在梯度下降框架下不起作用，那么索性直接把输入传到输出端，”强行“作为单位映射的部分，让可学习的网络作为另外一部分。

​	数据经过两条线：

* 和一般网络类似的经过两个卷积层再到达输出
* 另一条则是实现单位映射的直接连接的路线，被称为shorcut ，也就是捷径。这样做了之后，前面层的参数已经达到了一个很好的水平，那么再构建基本模块的时候，输入的信息通过shortcut得以一定程度的保留。

![](https://ws1.sinaimg.cn/mw690/007BZfbEly1g0gtd349jwj30xc0iowg7.jpg)

​	残差网络正是通过 区分直接求H(x)，让H(x) = F(x) + x,从而 F(x) = H(x) - x。在单位映射中，y = x相当于观测值，所以F(x) 对应残差，所以叫残差网络。

​	实际使用中，残差模块和Inception一样希望能够降低计算消耗。所以又提出 BottleNeck ，思路和Inception一样，就是先通过1x1卷积降维，然后是正常的3x3卷积层，最后再1x1卷积将维度和shorcut对应上。

​	再后来，作者将ReLU移到conv层之前，响应的shortcut不再经过ReLU，相当于输入输出直接相连。形成了**预激活残差单元**。

​	由于残差单元的出现，计算的性质从乘法变成了加法，计算变的更加简单、稳定。无论哪一层，更高层的梯度成分都可以直接传过去。

## 深度残差网络

​	比较经典的ResNet有 50层、101层、152层。

## 从集成的角度看待ResNet

​	把一个三个残差单元串联的图展开的示意图，每个单元都有两条路径，所以展开后有 2^3=8条路径。

​	整个网络像一个完全二叉树的结构，每一个高层节点都是由低层节点路径构成的一棵子树，由于路径非常多，所以等效于由低层路径构成的网络的一种集成。

​	网络真的越深越好吗？![](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicqq0P7gR2IDEM9NoBQ8iaMNIicSAg9jwwFMfwsu7XV1pO8ncHQfyYAFaHqMicOmRCtHAsR1OFiadRClw/640?wx_fmt=png)



​	啊，已经十二点半了，😪睡觉了。晚安。